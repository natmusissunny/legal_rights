"""
æµ‹è¯•ç½‘é¡µæŠ“å–æ¨¡å—
å®Œæ•´æµ‹è¯•ï¼šæŠ“å– â†’ æ¸…æ´— â†’ è§£æ
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root.parent))

from legal_rights.scraper import WebScraper, HTMLCleaner, ContentParser
from legal_rights.config import Config


async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„æŠ“å–æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•ç½‘é¡µæŠ“å–æ¨¡å—")
    print("=" * 80)

    # åˆå§‹åŒ–ç»„ä»¶
    scraper = WebScraper()
    cleaner = HTMLCleaner()
    parser = ContentParser()

    # 1. æŠ“å–ç½‘é¡µ
    print("\n[æ­¥éª¤1] æŠ“å–ç›®æ ‡ç½‘é¡µ")
    print("-" * 80)
    results = await scraper.fetch_target_urls(use_cache=True)

    # 2. æ¸…æ´—å’Œè§£æ
    print("\n[æ­¥éª¤2] æ¸…æ´—å’Œè§£æå†…å®¹")
    print("-" * 80)

    structured_contents = []

    for url, html in results.items():
        if not html:
            print(f"âŒ è·³è¿‡å¤±è´¥çš„URL: {url}")
            continue

        print(f"\nå¤„ç†: {url}")
        print("-" * 60)

        # æ¸…æ´—HTML
        cleaned_html, text = cleaner.clean_and_extract(html)
        print(f"  âœ… æ¸…æ´—å®Œæˆ")
        print(f"     åŸå§‹é•¿åº¦: {len(html):,} å­—ç¬¦")
        print(f"     æ¸…æ´—åé•¿åº¦: {len(cleaned_html):,} å­—ç¬¦")
        print(f"     çº¯æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")

        # è§£æç»“æ„
        structured = parser.parse(cleaned_html, url)
        structured_contents.append(structured)

        print(f"  âœ… è§£æå®Œæˆ")
        print(f"     æ ‡é¢˜: {structured.title}")
        print(f"     ç« èŠ‚æ•°: {len(structured.sections)}")
        print(f"     æŠ“å–æ—¶é—´: {structured.scraped_at.strftime('%Y-%m-%d %H:%M:%S')}")

        # æ˜¾ç¤ºç« èŠ‚ç»“æ„
        if structured.sections:
            print(f"\n  ğŸ“‹ ç« èŠ‚ç»“æ„:")
            _print_sections(structured.sections, indent=2)

    # 3. ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æœ")
    print("=" * 80)
    print(f"æ€»URLæ•°: {len(Config.TARGET_URLS)}")
    print(f"æŠ“å–æˆåŠŸ: {len([h for h in results.values() if h])}")
    print(f"è§£æå®Œæˆ: {len(structured_contents)}")

    total_sections = sum(len(sc.sections) for sc in structured_contents)
    print(f"æ€»ç« èŠ‚æ•°: {total_sections}")

    return structured_contents


def _print_sections(sections, indent=0):
    """é€’å½’æ‰“å°ç« èŠ‚ç»“æ„"""
    for section in sections:
        prefix = "    " * indent
        content_preview = section.content[:80].replace('\n', ' ') if section.content else "(æ— å†…å®¹)"
        print(f"{prefix}â”œâ”€ [Lv{section.level}] {section.title}")
        print(f"{prefix}â”‚  {content_preview}...")

        if section.subsections:
            _print_sections(section.subsections, indent + 1)


async def test_single_url():
    """æµ‹è¯•å•ä¸ªURL"""
    test_url = Config.TARGET_URLS[0]

    print(f"ğŸ§ª æµ‹è¯•å•ä¸ªURL: {test_url}")
    print("=" * 80)

    scraper = WebScraper()
    cleaner = HTMLCleaner()
    parser = ContentParser()

    # æŠ“å–
    print("\n[1] æŠ“å–ç½‘é¡µ...")
    html = await scraper.fetch(test_url, use_cache=True)

    if not html:
        print("âŒ æŠ“å–å¤±è´¥")
        return

    print(f"âœ… æŠ“å–æˆåŠŸ ({len(html):,} å­—ç¬¦)")

    # æ¸…æ´—
    print("\n[2] æ¸…æ´—HTML...")
    cleaned_html, text = cleaner.clean_and_extract(html)
    print(f"âœ… æ¸…æ´—å®Œæˆ")
    print(f"   æ¸…æ´—å: {len(cleaned_html):,} å­—ç¬¦")
    print(f"   çº¯æ–‡æœ¬: {len(text):,} å­—ç¬¦")

    # è§£æ
    print("\n[3] è§£æå†…å®¹...")
    structured = parser.parse(cleaned_html, test_url)
    print(f"âœ… è§£æå®Œæˆ")
    print(f"   æ ‡é¢˜: {structured.title}")
    print(f"   ç« èŠ‚æ•°: {len(structured.sections)}")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æ„
    print("\n[4] å†…å®¹é¢„è§ˆ:")
    print("-" * 80)
    print(f"æ ‡é¢˜: {structured.title}")
    print(f"URL: {structured.url}")
    print(f"æŠ“å–æ—¶é—´: {structured.scraped_at}")
    print()

    if structured.sections:
        print("ç« èŠ‚ç»“æ„:")
        _print_sections(structured.sections, indent=0)
    else:
        print("(æœªæ‰¾åˆ°ç« èŠ‚ç»“æ„)")

    # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
    print("\n[5] æ–‡æœ¬é¢„è§ˆ (å‰500å­—):")
    print("-" * 80)
    print(text[:500])
    if len(text) > 500:
        print("...")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æµ‹è¯•ç½‘é¡µæŠ“å–æ¨¡å—")
    parser.add_argument(
        '--mode',
        choices=['full', 'single'],
        default='full',
        help='æµ‹è¯•æ¨¡å¼: full=å®Œæ•´æµ‹è¯•, single=å•ä¸ªURL'
    )

    args = parser.parse_args()

    if args.mode == 'full':
        asyncio.run(test_full_pipeline())
    else:
        asyncio.run(test_single_url())


if __name__ == "__main__":
    main()
