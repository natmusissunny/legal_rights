# ðŸ”„ LiteLLM é›†æˆæŒ‡å—

**é€šè¿‡ LiteLLM ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å¤§æ¨¡åž‹**

---

## ðŸŽ¯ ä»€ä¹ˆæ˜¯ LiteLLMï¼Ÿ

**LiteLLM** = å¤§æ¨¡åž‹çš„"ç»Ÿä¸€æŽ¥å£"

æ”¯æŒ 100+ æ¨¡åž‹æä¾›å•†ï¼š
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Google (Gemini)
- é˜¿é‡Œäº‘ï¼ˆé€šä¹‰åƒé—®ï¼‰
- æ™ºè°±AIï¼ˆGLMï¼‰
- DeepSeek
- æ›´å¤š...

**å®˜æ–¹æ–‡æ¡£**: https://docs.litellm.ai/

---

## ðŸŒŸ ä¸ºä»€ä¹ˆè¦ç”¨ LiteLLMï¼Ÿ

### å¯¹æ¯”ï¼šä¸ä½¿ç”¨ LiteLLM

```python
# éœ€è¦ä¸ºæ¯ä¸ªæ¨¡åž‹å†™é€‚é…ä»£ç 
if model == "claude":
    from anthropic import Anthropic
    client = Anthropic(api_key=key)
    response = client.messages.create(...)

elif model == "qwen":
    from dashscope import Generation
    response = Generation.call(...)

elif model == "deepseek":
    from openai import OpenAI
    client = OpenAI(api_key=key, base_url="...")
    response = client.chat.completions.create(...)
```

### ä½¿ç”¨ LiteLLM

```python
# ç»Ÿä¸€æŽ¥å£ï¼Œä¸€è¡Œä»£ç æžå®š
from litellm import completion

response = completion(
    model="claude-3-opus",  # æˆ– gpt-4 / qwen-max
    messages=[{"role": "user", "content": "..."}]
)
```

### LiteLLM çš„ä¼˜åŠ¿

1. âœ… **ç»Ÿä¸€æŽ¥å£** - ä¸€å¥—ä»£ç æ”¯æŒæ‰€æœ‰æ¨¡åž‹
2. âœ… **è´Ÿè½½å‡è¡¡** - è‡ªåŠ¨åœ¨å¤šä¸ªæ¨¡åž‹é—´åˆ†é…è¯·æ±‚
3. âœ… **å¤±è´¥é‡è¯•** - è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚
4. âœ… **æˆæœ¬è¿½è¸ª** - ç»Ÿè®¡æ¯ä¸ªæ¨¡åž‹çš„è°ƒç”¨æˆæœ¬
5. âœ… **é€ŸçŽ‡é™åˆ¶** - é¿å…è¶…è¿‡APIé™é¢
6. âœ… **æ¨¡åž‹è·¯ç”±** - æ ¹æ®è§„åˆ™è‡ªåŠ¨é€‰æ‹©æ¨¡åž‹

---

## ðŸš€ ä¸¤ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼1: ç›´æŽ¥ä½¿ç”¨ LiteLLMï¼ˆç®€å•ï¼‰

**é€‚åˆ**: å•æœºä½¿ç”¨ï¼Œä¸éœ€è¦ä»£ç†æœåŠ¡å™¨

```bash
# 1. å®‰è£…
pip install litellm

# 2. é…ç½® .env
echo "LITELLM_MODEL=claude-3-opus-20240229" >> .env
echo "LLM_MODE=litellm" >> .env

# åŒæ—¶é…ç½®å¯¹åº”æ¨¡åž‹çš„APIå¯†é’¥
echo "ANTHROPIC_API_KEY=your-claude-key" >> .env

# 3. ä½¿ç”¨
python -m legal_rights ask "é—®é¢˜"
```

### æ–¹å¼2: ä½¿ç”¨ LiteLLM ä»£ç†ï¼ˆæŽ¨èï¼‰

**é€‚åˆ**: éœ€è¦è´Ÿè½½å‡è¡¡ã€æˆæœ¬è¿½è¸ªã€å¤šç”¨æˆ·ç­‰é«˜çº§åŠŸèƒ½

```bash
# 1. å®‰è£…
pip install 'litellm[proxy]'

# 2. åˆ›å»ºé…ç½®æ–‡ä»¶
cat > litellm_config.yaml << EOF
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: your-openai-key

  - model_name: claude
    litellm_params:
      model: claude-3-opus-20240229
      api_key: your-claude-key

  - model_name: qwen
    litellm_params:
      model: qwen/qwen-max
      api_key: your-qwen-key
EOF

# 3. å¯åŠ¨ LiteLLM ä»£ç†
litellm --config litellm_config.yaml --port 4000

# 4. é…ç½®é¡¹ç›®
echo "LITELLM_MODEL=gpt-4" >> .env
echo "LITELLM_API_BASE=http://localhost:4000" >> .env
echo "LLM_MODE=litellm" >> .env

# 5. ä½¿ç”¨
python -m legal_rights ask "é—®é¢˜"
```

---

## ðŸ“ é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹1: ç›´æŽ¥ä½¿ç”¨ Claude

**.env æ–‡ä»¶**:
```env
# ä½¿ç”¨ LiteLLM è°ƒç”¨ Claude
LITELLM_MODEL=claude-3-opus-20240229
LLM_MODE=litellm

# Claude APIå¯†é’¥
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx

# Embeddingï¼ˆä»éœ€é…ç½®ï¼‰
ZHIPUAI_API_KEY=xxxxx.xxxxx
```

**åŽŸç†**:
- LiteLLM è¯»å– `ANTHROPIC_API_KEY`
- è‡ªåŠ¨è°ƒç”¨ Claude API

---

### ç¤ºä¾‹2: ä½¿ç”¨ LiteLLM ä»£ç†

**litellm_config.yaml**:
```yaml
model_list:
  # OpenAI GPT-4
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY

  # Claude Opus
  - model_name: claude-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  # é€šä¹‰åƒé—®
  - model_name: qwen
    litellm_params:
      model: qwen/qwen-max
      api_key: os.environ/DASHSCOPE_API_KEY

# è´Ÿè½½å‡è¡¡é…ç½®ï¼ˆå¯é€‰ï¼‰
router_settings:
  routing_strategy: simple-shuffle  # éšæœºé€‰æ‹©

# é€ŸçŽ‡é™åˆ¶ï¼ˆå¯é€‰ï¼‰
general_settings:
  max_parallel_requests: 10
```

**å¯åŠ¨ä»£ç†**:
```bash
# è®¾ç½®çŽ¯å¢ƒå˜é‡
export OPENAI_API_KEY=sk-xxxxx
export ANTHROPIC_API_KEY=sk-ant-xxxxx
export DASHSCOPE_API_KEY=sk-xxxxx

# å¯åŠ¨
litellm --config litellm_config.yaml --port 4000
```

**.env æ–‡ä»¶**:
```env
# ä½¿ç”¨ LiteLLM ä»£ç†
LITELLM_MODEL=gpt-4
LITELLM_API_BASE=http://localhost:4000
LLM_MODE=litellm

# Embedding
ZHIPUAI_API_KEY=xxxxx.xxxxx
```

---

### ç¤ºä¾‹3: è´Ÿè½½å‡è¡¡ + å¤±è´¥é‡è¯•

**litellm_config.yaml**:
```yaml
model_list:
  # ä¸»åŠ›æ¨¡åž‹ï¼šGPT-4
  - model_name: gpt-4-primary
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY

  # å¤‡ç”¨æ¨¡åž‹ï¼šClaude
  - model_name: gpt-4-fallback
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

# è·¯ç”±è®¾ç½®
router_settings:
  routing_strategy: usage-based-routing-v2
  num_retries: 3  # å¤±è´¥é‡è¯•3æ¬¡
  timeout: 60     # è¶…æ—¶æ—¶é—´60ç§’
  fallbacks:
    - gpt-4-primary
    - gpt-4-fallback  # GPT-4å¤±è´¥åˆ™ç”¨Claude

# æˆæœ¬è¿½è¸ª
litellm_settings:
  success_callback: ["langfuse"]  # å¯é€‰ï¼šå‘é€åˆ°Langfuse
  failure_callback: ["langfuse"]
```

---

## ðŸ”§ é«˜çº§åŠŸèƒ½

### 1. æˆæœ¬è¿½è¸ª

```yaml
# litellm_config.yaml
litellm_settings:
  success_callback: ["langfuse", "supabase"]

langfuse_params:
  public_key: your-key
  secret_key: your-secret
```

å¯åŠ¨åŽè®¿é—®: http://localhost:4000/metrics

æŸ¥çœ‹ï¼š
- æ€»è¯·æ±‚æ•°
- æˆåŠŸ/å¤±è´¥çŽ‡
- æ¯ä¸ªæ¨¡åž‹çš„æˆæœ¬
- å“åº”æ—¶é—´

### 2. è´Ÿè½½å‡è¡¡

```yaml
router_settings:
  routing_strategy: least-busy  # é€‰æ‹©æœ€ç©ºé—²çš„æ¨¡åž‹
  # æˆ–
  routing_strategy: simple-shuffle  # éšæœºé€‰æ‹©
  # æˆ–
  routing_strategy: usage-based-routing-v2  # åŸºäºŽä½¿ç”¨é‡
```

### 3. æ¨¡åž‹è·¯ç”±

```yaml
# æ ¹æ®è¯·æ±‚å†…å®¹é€‰æ‹©æ¨¡åž‹
router_settings:
  model_group_alias:
    "cheap": ["gpt-3.5-turbo", "qwen-turbo"]
    "expensive": ["gpt-4", "claude-3-opus"]

# ä½¿ç”¨æ—¶æŒ‡å®šç»„
# completion(model="cheap", messages=[...])
```

### 4. é€ŸçŽ‡é™åˆ¶

```yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: xxx
      rpm: 60  # æ¯åˆ†é’Ÿ60æ¬¡è¯·æ±‚
      tpm: 100000  # æ¯åˆ†é’Ÿ100k tokens
```

---

## ðŸŽ¯ ä¸ŽçŽ°æœ‰åŠŸèƒ½å¯¹æ¯”

### ä¸ä½¿ç”¨ LiteLLMï¼ˆçŽ°æœ‰æ–¹å¼ï¼‰

```env
# éœ€è¦é…ç½®å…·ä½“æ¨¡åž‹çš„å¯†é’¥
DEEPSEEK_API_KEY=sk-xxxxx
ZHIPUAI_API_KEY=xxxxx.xxxxx
LLM_MODE=auto
```

**ç‰¹ç‚¹**:
- âœ… ç®€å•ç›´æŽ¥
- âœ… æ— éœ€é¢å¤–æœåŠ¡
- âŒ æ¯ä¸ªæ¨¡åž‹éœ€è¦å•ç‹¬é€‚é…
- âŒ æ— æ³•è´Ÿè½½å‡è¡¡
- âŒ æ— æ³•æˆæœ¬è¿½è¸ª

### ä½¿ç”¨ LiteLLM

```env
# ç»Ÿä¸€æŽ¥å£
LITELLM_MODEL=gpt-4
LITELLM_API_BASE=http://localhost:4000
LLM_MODE=litellm

# åªéœ€é…ç½® Embedding
ZHIPUAI_API_KEY=xxxxx.xxxxx
```

**ç‰¹ç‚¹**:
- âœ… ç»Ÿä¸€æŽ¥å£
- âœ… æ”¯æŒè´Ÿè½½å‡è¡¡
- âœ… æ”¯æŒæˆæœ¬è¿½è¸ª
- âœ… æ”¯æŒå¤±è´¥é‡è¯•
- âš ï¸ éœ€è¦å¯åŠ¨ä»£ç†æœåŠ¡ï¼ˆæ–¹å¼2ï¼‰

---

## ðŸ“Š å®žé™…ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: ä¸ªäººå¼€å‘ï¼ˆç›´æŽ¥ä½¿ç”¨ï¼‰

```bash
# å®‰è£…
pip install litellm

# é…ç½®
cat >> .env << EOF
LITELLM_MODEL=claude-3-opus-20240229
LLM_MODE=litellm
ANTHROPIC_API_KEY=your-key
ZHIPUAI_API_KEY=your-key
EOF

# ä½¿ç”¨
python -m legal_rights ask "å…¬å¸æ¶æ„è¾žé€€æ€Žä¹ˆåŠžï¼Ÿ"
```

**ä¼˜åŠ¿**: æ— éœ€å¯åŠ¨ä»£ç†ï¼Œé…ç½®ç®€å•

---

### åœºæ™¯2: å›¢é˜Ÿåä½œï¼ˆä½¿ç”¨ä»£ç†ï¼‰

```bash
# 1. æœåŠ¡å™¨ä¸Šå¯åŠ¨ LiteLLM ä»£ç†
# server.yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: team-openai-key
  - model_name: claude
    litellm_params:
      model: claude-3-opus-20240229
      api_key: team-claude-key

litellm --config server.yaml --port 4000

# 2. å›¢é˜Ÿæˆå‘˜åªéœ€é…ç½®ä»£ç†åœ°å€
# .env
LITELLM_MODEL=gpt-4
LITELLM_API_BASE=http://team-server:4000
LLM_MODE=litellm
ZHIPUAI_API_KEY=my-embedding-key
```

**ä¼˜åŠ¿**:
- ç»Ÿä¸€ç®¡ç†APIå¯†é’¥
- æˆæœ¬è¿½è¸ªå’ŒæŽ§åˆ¶
- è´Ÿè½½å‡è¡¡

---

### åœºæ™¯3: æ··åˆä½¿ç”¨

```bash
# å¼€å‘çŽ¯å¢ƒï¼šç›´æŽ¥è°ƒç”¨ï¼ˆä¾¿å®œï¼‰
DEEPSEEK_API_KEY=my-key
LLM_MODE=deepseek

# ç”Ÿäº§çŽ¯å¢ƒï¼šLiteLLMä»£ç†ï¼ˆç¨³å®šï¼‰
LITELLM_MODEL=gpt-4
LITELLM_API_BASE=http://prod-server:4000
LLM_MODE=litellm
```

**ä¼˜åŠ¿**: çµæ´»åˆ‡æ¢ï¼Œå¼€å‘çœé’±ï¼Œç”Ÿäº§ç¨³å®š

---

## ðŸ” å®Œæ•´é…ç½®ç¤ºä¾‹

### ç”Ÿäº§çº§ LiteLLM é…ç½®

```yaml
# litellm_config.yaml
model_list:
  # ä¸»åŠ›ï¼šOpenAI
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY
      rpm: 500
      tpm: 150000

  # å¤‡ç”¨1ï¼šClaude
  - model_name: claude-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      rpm: 400
      tpm: 100000

  # å¤‡ç”¨2ï¼šé€šä¹‰åƒé—®ï¼ˆä¾¿å®œï¼‰
  - model_name: qwen-max
    litellm_params:
      model: qwen/qwen-max
      api_key: os.environ/DASHSCOPE_API_KEY
      rpm: 1000
      tpm: 200000

# è·¯ç”±ç­–ç•¥
router_settings:
  routing_strategy: usage-based-routing-v2
  num_retries: 3
  timeout: 60
  fallbacks:
    - gpt-4
    - claude-opus
    - qwen-max  # æœ€åŽé™çº§åˆ°ä¾¿å®œæ¨¡åž‹

# æˆæœ¬è¿½è¸ª
litellm_settings:
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  set_verbose: true

# æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
general_settings:
  database_url: postgresql://user:pass@localhost/litellm

# UIè®¿é—®æŽ§åˆ¶ï¼ˆå¯é€‰ï¼‰
environment_variables:
  LITELLM_MASTER_KEY: your-secret-key
```

**å¯åŠ¨**:
```bash
litellm --config litellm_config.yaml \
  --port 4000 \
  --num_workers 4 \
  --detailed_debug
```

**è®¿é—®**:
- API: http://localhost:4000
- UI: http://localhost:4000/ui
- Metrics: http://localhost:4000/metrics

---

## ðŸ“š å¸¸è§é—®é¢˜

### Q1: å¿…é¡»ä½¿ç”¨ LiteLLM å—ï¼Ÿ

**ä¸å¿…é¡»**ã€‚LiteLLM æ˜¯å¯é€‰åŠŸèƒ½ã€‚

- âœ… ä½¿ç”¨ LiteLLMï¼šé€‚åˆéœ€è¦é«˜çº§åŠŸèƒ½çš„ç”¨æˆ·
- âœ… ä¸ä½¿ç”¨ï¼šç»§ç»­ç”¨çŽ°æœ‰çš„ç›´æŽ¥è°ƒç”¨æ–¹å¼

### Q2: LiteLLM ä¼šå¢žåŠ å»¶è¿Ÿå—ï¼Ÿ

**å‡ ä¹Žæ²¡æœ‰**ã€‚LiteLLM åªæ˜¯ä¸€ä¸ªè½»é‡çº§ä»£ç†ï¼Œå»¶è¿Ÿå¢žåŠ  < 10msã€‚

### Q3: å¯ä»¥åŒæ—¶ç”¨ LiteLLM å’Œç›´æŽ¥è°ƒç”¨å—ï¼Ÿ

**å¯ä»¥**ã€‚é€šè¿‡ `LLM_MODE` åˆ‡æ¢ï¼š

```env
# ä½¿ç”¨ LiteLLM
LLM_MODE=litellm

# ä½¿ç”¨ DeepSeek
LLM_MODE=deepseek
```

### Q4: LiteLLM æ”¯æŒ Embedding å—ï¼Ÿ

**æ”¯æŒ**ï¼Œä½†æœ¬é¡¹ç›®çš„ Embedding ä»ä½¿ç”¨ç›´æŽ¥è°ƒç”¨æ–¹å¼ã€‚

åŽŸå› ï¼šEmbedding è°ƒç”¨ç®€å•ï¼Œä¸éœ€è¦è´Ÿè½½å‡è¡¡ç­‰é«˜çº§åŠŸèƒ½ã€‚

### Q5: æˆæœ¬ä¼šå¢žåŠ å—ï¼Ÿ

**ä¸ä¼š**ã€‚LiteLLM æœ¬èº«å…è´¹å¼€æºï¼Œåªäº§ç”Ÿæ¨¡åž‹è°ƒç”¨æˆæœ¬ã€‚

---

## âœ… å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•æ–¹å¼ï¼ˆç›´æŽ¥ä½¿ç”¨ï¼‰

```bash
# 1. å®‰è£…
pip install litellm

# 2. é…ç½®
cat >> .env << EOF
LITELLM_MODEL=claude-3-opus-20240229
LLM_MODE=litellm
ANTHROPIC_API_KEY=your-claude-key
ZHIPUAI_API_KEY=your-zhipu-key
EOF

# 3. æµ‹è¯•
python -m legal_rights config

# 4. ä½¿ç”¨
python -m legal_rights ask "æµ‹è¯•é—®é¢˜"
```

### æŽ¨èæ–¹å¼ï¼ˆä½¿ç”¨ä»£ç†ï¼‰

```bash
# 1. å®‰è£…
pip install 'litellm[proxy]'

# 2. åˆ›å»ºé…ç½®
cat > litellm_config.yaml << 'EOF'
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: your-key
EOF

# 3. å¯åŠ¨ä»£ç†
litellm --config litellm_config.yaml --port 4000

# 4. é…ç½®é¡¹ç›®
cat >> .env << EOF
LITELLM_MODEL=gpt-4
LITELLM_API_BASE=http://localhost:4000
LLM_MODE=litellm
ZHIPUAI_API_KEY=your-zhipu-key
EOF

# 5. ä½¿ç”¨
python -m legal_rights ask "æµ‹è¯•é—®é¢˜"
```

---

## ðŸ”— ç›¸å…³èµ„æº

- **LiteLLM å®˜æ–¹æ–‡æ¡£**: https://docs.litellm.ai/
- **æ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨**: https://docs.litellm.ai/docs/providers
- **LiteLLM GitHub**: https://github.com/BerriAI/litellm
- **LiteLLM ä»£ç†æ–‡æ¡£**: https://docs.litellm.ai/docs/proxy/quick_start

---

**æ›´æ–°æ—¥æœŸ**: 2026-02-06
**ç‰ˆæœ¬**: v1.1.0
