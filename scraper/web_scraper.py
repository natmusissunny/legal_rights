"""
ç½‘é¡µæŠ“å–å™¨
ä½¿ç”¨ httpx å¼‚æ­¥æŠ“å–ç›®æ ‡ç½‘é¡µï¼Œæ”¯æŒç¼“å­˜å’Œé‡è¯•
"""
import asyncio
import hashlib
from pathlib import Path
from typing import Optional, List
from datetime import datetime
import httpx
from ..config import Config


class WebScraper:
    """ç½‘é¡µæŠ“å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.cache_dir = Config.CACHE_DIR
        self.rate_limit = Config.RATE_LIMIT_PER_SECOND
        self.user_agent = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
        self._last_request_time = 0

    async def _wait_for_rate_limit(self):
        """ç­‰å¾…é€Ÿç‡é™åˆ¶"""
        current_time = asyncio.get_event_loop().time()
        time_since_last = current_time - self._last_request_time
        min_interval = 1.0 / self.rate_limit

        if time_since_last < min_interval:
            await asyncio.sleep(min_interval - time_since_last)

        self._last_request_time = asyncio.get_event_loop().time()

    def _get_cache_path(self, url: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨ URL çš„ MD5 ä½œä¸ºæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.cache_dir / f"{url_hash}.html"

    def _get_cache_metadata_path(self, url: str) -> Path:
        """è·å–ç¼“å­˜å…ƒæ•°æ®è·¯å¾„"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.cache_dir / f"{url_hash}.meta"

    def _load_from_cache(self, url: str) -> Optional[str]:
        """ä»ç¼“å­˜åŠ è½½HTML"""
        cache_path = self._get_cache_path(url)
        if cache_path.exists():
            try:
                return cache_path.read_text(encoding='utf-8')
            except Exception as e:
                print(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                return None
        return None

    def _save_to_cache(self, url: str, html: str):
        """ä¿å­˜HTMLåˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        meta_path = self._get_cache_metadata_path(url)

        try:
            # ä¿å­˜ HTML å†…å®¹
            cache_path.write_text(html, encoding='utf-8')

            # ä¿å­˜å…ƒæ•°æ®
            metadata = f"url={url}\ntimestamp={datetime.now().isoformat()}\n"
            meta_path.write_text(metadata, encoding='utf-8')

            print(f"âœ… å·²ç¼“å­˜: {url}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    async def fetch(
        self,
        url: str,
        use_cache: bool = True,
        max_retries: int = 3
    ) -> Optional[str]:
        """
        æŠ“å–å•ä¸ªç½‘é¡µ

        Args:
            url: ç›®æ ‡URL
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_html = self._load_from_cache(url)
            if cached_html:
                print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {url}")
                return cached_html

        # æŠ“å–ç½‘é¡µ
        print(f"ğŸŒ æŠ“å–ä¸­: {url}")

        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }

        for attempt in range(max_retries):
            try:
                # ç­‰å¾…é€Ÿç‡é™åˆ¶
                await self._wait_for_rate_limit()

                async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                    response = await client.get(url, headers=headers)
                    response.raise_for_status()

                    # å°è¯•å¤šç§ç¼–ç 
                    html = None
                    for encoding in ['utf-8', 'gb2312', 'gbk']:
                        try:
                            html = response.content.decode(encoding)
                            break
                        except UnicodeDecodeError:
                            continue

                    if html is None:
                        # ä½¿ç”¨ chardet è‡ªåŠ¨æ£€æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        try:
                            import chardet
                            detected = chardet.detect(response.content)
                            html = response.content.decode(detected['encoding'])
                        except:
                            # æœ€åä½¿ç”¨é»˜è®¤ç¼–ç å¹¶å¿½ç•¥é”™è¯¯
                            html = response.content.decode('utf-8', errors='ignore')

                    # ä¿å­˜åˆ°ç¼“å­˜
                    self._save_to_cache(url, html)

                    print(f"âœ… æŠ“å–æˆåŠŸ: {url} ({len(html)} å­—ç¬¦)")
                    return html

            except httpx.HTTPStatusError as e:
                print(f"âŒ HTTPé”™è¯¯ {e.response.status_code}: {url}")
                if e.response.status_code in [404, 403, 401]:
                    # ä¸é‡è¯•è¿™äº›é”™è¯¯
                    return None
            except httpx.TimeoutException:
                print(f"â±ï¸  è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {url}")
            except Exception as e:
                print(f"âŒ æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"â³ ç­‰å¾… {wait_time}s åé‡è¯•...")
                await asyncio.sleep(wait_time)

        print(f"âŒ æŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
        return None

    async def fetch_all(
        self,
        urls: List[str],
        use_cache: bool = True,
        max_retries: int = 3
    ) -> dict[str, Optional[str]]:
        """
        æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ

        Args:
            urls: URLåˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            å­—å…¸ {url: html_content}
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} ä¸ªç½‘é¡µ...")
        print("=" * 60)

        tasks = [self.fetch(url, use_cache, max_retries) for url in urls]
        results = await asyncio.gather(*tasks)

        result_dict = dict(zip(urls, results))

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for html in results if html is not None)
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æŠ“å–å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)}")

        return result_dict

    async def fetch_target_urls(
        self,
        use_cache: bool = True
    ) -> dict[str, Optional[str]]:
        """
        æŠ“å–é…ç½®ä¸­çš„ç›®æ ‡URL

        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            å­—å…¸ {url: html_content}
        """
        return await self.fetch_all(Config.TARGET_URLS, use_cache)


async def main():
    """æµ‹è¯•å‡½æ•°"""
    scraper = WebScraper()

    # æµ‹è¯•æŠ“å–ç›®æ ‡URL
    results = await scraper.fetch_target_urls(use_cache=True)

    print("\nğŸ“‹ æŠ“å–ç»“æœ:")
    print("=" * 60)
    for url, html in results.items():
        if html:
            print(f"âœ… {url}")
            print(f"   é•¿åº¦: {len(html)} å­—ç¬¦")
        else:
            print(f"âŒ {url}")
            print(f"   çŠ¶æ€: å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(main())
