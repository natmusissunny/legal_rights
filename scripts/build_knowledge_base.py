"""
ä¸€é”®æ„å»ºçŸ¥è¯†åº“è„šæœ¬
è‡ªåŠ¨åŒ–å®Œæˆä»æŠ“å–åˆ°ç´¢å¼•çš„å…¨éƒ¨æµç¨‹
"""
import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root.parent))

from legal_rights.config import Config
from legal_rights.scraper import WebScraper, HTMLCleaner, ContentParser
from legal_rights.knowledge import TextGenerator, VectorIndexer


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(title.center(80))
    print("=" * 80)


def print_step(step_num: int, total_steps: int, title: str):
    """æ‰“å°æ­¥éª¤æ ‡é¢˜"""
    print(f"\n[æ­¥éª¤ {step_num}/{total_steps}] {title}")
    print("-" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    print_header("ğŸ—ï¸  æ³•å¾‹ç»´æƒçŸ¥è¯†åº“æ„å»ºå·¥å…·")

    # æ£€æŸ¥APIå¯†é’¥
    print("\næ£€æŸ¥é…ç½®...")
    if not Config.OPENAI_API_KEY:
        print("âŒ é”™è¯¯: æœªé…ç½® OPENAI_API_KEY")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® API å¯†é’¥")
        sys.exit(1)

    print("âœ… é…ç½®æ£€æŸ¥é€šè¿‡")

    # æ­¥éª¤1: æŠ“å–ç½‘é¡µ
    print_step(1, 4, "æŠ“å–ç½‘é¡µå†…å®¹")

    scraper = WebScraper()
    results = await scraper.fetch_target_urls(use_cache=True)

    successful_urls = [url for url, html in results.items() if html]
    if not successful_urls:
        print("\nâŒ æ‰€æœ‰ç½‘é¡µæŠ“å–å¤±è´¥")
        print("æç¤º: ç½‘ç«™å¯èƒ½æœ‰åçˆ¬è™«ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½HTMLåˆ° data/cache/")
        print("æˆ–è¿è¡Œ: python -m legal_rights build-kb --skip-scrape")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜
        cache_files = list(Config.CACHE_DIR.glob("*.html"))
        if cache_files:
            print(f"\nâœ… å‘ç° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œå°†ç»§ç»­å¤„ç†")
        else:
            sys.exit(1)

    # æ­¥éª¤2: è§£æå†…å®¹
    print_step(2, 4, "è§£æå’Œæ¸…æ´—å†…å®¹")

    cleaner = HTMLCleaner()
    parser = ContentParser()

    # è¯»å–æ‰€æœ‰HTMLæ–‡ä»¶
    cache_files = list(Config.CACHE_DIR.glob("*.html"))
    print(f"æ‰¾åˆ° {len(cache_files)} ä¸ªHTMLæ–‡ä»¶")

    structured_contents = []

    for i, cache_file in enumerate(cache_files, 1):
        print(f"\n  [{i}/{len(cache_files)}] å¤„ç†: {cache_file.name}")

        try:
            html = cache_file.read_text(encoding='utf-8')
            cleaned_html, text = cleaner.clean_and_extract(html)

            # ä»å…ƒæ•°æ®æ–‡ä»¶è¯»å–URLï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            meta_file = cache_file.with_suffix('.meta')
            url = f"file://{cache_file}"
            if meta_file.exists():
                meta_content = meta_file.read_text(encoding='utf-8')
                for line in meta_content.split('\n'):
                    if line.startswith('url='):
                        url = line.split('=', 1)[1].strip()
                        break

            # æå–æ ‡é¢˜
            title = f"æ³•å¾‹æ–‡æ¡£ {i}"
            temp_parser = ContentParser()
            temp_soup = __import__('bs4').BeautifulSoup(cleaned_html, 'lxml')
            extracted_title = temp_parser._extract_title(temp_soup)
            if extracted_title != "æœªå‘½åæ–‡æ¡£":
                title = extracted_title

            structured = parser.parse(
                html=cleaned_html,
                url=url,
                title=title
            )

            structured_contents.append(structured)
            print(f"      âœ… è§£æå®Œæˆ")
            print(f"         æ ‡é¢˜: {title}")
            print(f"         ç« èŠ‚: {len(structured.sections)}")

        except Exception as e:
            print(f"      âŒ è§£æå¤±è´¥: {e}")
            continue

    if not structured_contents:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè§£æä»»ä½•å†…å®¹")
        sys.exit(1)

    print(f"\nâœ… æˆåŠŸè§£æ {len(structured_contents)} ä¸ªæ–‡æ¡£")

    # æ­¥éª¤3: ç”Ÿæˆæ–‡æ¡£
    print_step(3, 4, "ç”ŸæˆMarkdownæ–‡æ¡£")

    generator = TextGenerator()
    doc_paths = generator.generate_batch(structured_contents, format='md')

    print(f"\nâœ… ç”Ÿæˆäº† {len(doc_paths)} ä¸ªMarkdownæ–‡æ¡£")

    # æ­¥éª¤4: æ„å»ºå‘é‡ç´¢å¼•
    print_step(4, 4, "æ„å»ºå‘é‡ç´¢å¼•")

    try:
        indexer = VectorIndexer()
        indexer.build_index(structured_contents, show_progress=True)
        indexer.save_index()
    except Exception as e:
        print(f"\nâŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # å®Œæˆ
    print_header("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    print("\nğŸ“Š æ„å»ºæ‘˜è¦:")
    print(f"  - å¤„ç†æ–‡æ¡£: {len(structured_contents)}")
    print(f"  - ç”Ÿæˆæ–‡æ¡£: {len(doc_paths)}")
    print(f"  - ç´¢å¼•æ–‡æ¡£å—: {len(indexer.documents)}")
    print(f"  - å‘é‡ç»´åº¦: {indexer.dimension}")

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  python -m legal_rights ask \"ä½ çš„é—®é¢˜\"")
    print("  python -m legal_rights chat")

    return True


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
