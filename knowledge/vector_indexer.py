"""
å‘é‡ç´¢å¼•æ„å»ºå™¨
ä½¿ç”¨ FAISS æ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•
"""
import json
import pickle
from pathlib import Path
from typing import List, Optional
import numpy as np
import faiss

from ..models import Document, StructuredContent
from ..config import Config
from .embedding_client import EmbeddingClient
from .document_chunker import DocumentChunker


class VectorIndexer:
    """å‘é‡ç´¢å¼•æ„å»ºå™¨"""

    def __init__(
        self,
        embedding_client: Optional[EmbeddingClient] = None,
        chunker: Optional[DocumentChunker] = None
    ):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨

        Args:
            embedding_client: Embeddingå®¢æˆ·ç«¯
            chunker: æ–‡æ¡£åˆ†å—å™¨
        """
        self.embedding_client = embedding_client or EmbeddingClient()
        self.chunker = chunker or DocumentChunker()
        self.index: Optional[faiss.Index] = None
        self.documents: List[Document] = []
        self.dimension = self.embedding_client.get_embedding_dimension()

    def build_index(
        self,
        contents: List[StructuredContent],
        show_progress: bool = True
    ):
        """
        æ„å»ºå‘é‡ç´¢å¼•

        Args:
            contents: ç»“æ„åŒ–å†…å®¹åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        """
        if show_progress:
            print("\nğŸ—ï¸  æ„å»ºå‘é‡ç´¢å¼•")
            print("=" * 70)

        # 1. åˆ†å‰²æ–‡æ¡£
        if show_progress:
            print("\n[æ­¥éª¤1/3] åˆ†å‰²æ–‡æ¡£")
        self.documents = self.chunker.chunk_batch(contents, show_progress)

        if not self.documents:
            print("âŒ æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•")
            return

        # 2. ç”Ÿæˆå‘é‡
        if show_progress:
            print(f"\n[æ­¥éª¤2/3] ç”ŸæˆEmbedding")

        texts = [doc.content for doc in self.documents]
        embeddings = self.embedding_client.embed_batch(
            texts,
            batch_size=100,
            show_progress=show_progress
        )

        # å°†å‘é‡æ·»åŠ åˆ°æ–‡æ¡£
        for doc, embedding in zip(self.documents, embeddings):
            doc.embedding = embedding

        # 3. æ„å»ºFAISSç´¢å¼•
        if show_progress:
            print(f"\n[æ­¥éª¤3/3] æ„å»ºFAISSç´¢å¼•")
            print("-" * 70)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        vectors = np.array(embeddings, dtype=np.float32)

        # åˆ›å»ºFAISSç´¢å¼•ï¼ˆä½¿ç”¨L2è·ç¦»ï¼‰
        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(vectors)

        if show_progress:
            print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
            print(f"   æ–‡æ¡£æ•°: {len(self.documents)}")
            print(f"   å‘é‡ç»´åº¦: {self.dimension}")
            print(f"   ç´¢å¼•ç±»å‹: IndexFlatL2")

    def save_index(
        self,
        index_path: Optional[Path] = None,
        metadata_path: Optional[Path] = None
    ):
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶

        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        if self.index is None:
            raise ValueError("Index not built yet")

        # é»˜è®¤è·¯å¾„
        if index_path is None:
            index_path = Config.VECTORS_DIR / "index.faiss"
        if metadata_path is None:
            metadata_path = Config.VECTORS_DIR / "metadata.pkl"

        print(f"\nğŸ’¾ ä¿å­˜ç´¢å¼•")
        print("-" * 70)

        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, str(index_path))
        print(f"âœ… FAISSç´¢å¼•å·²ä¿å­˜: {index_path}")

        # ä¿å­˜å…ƒæ•°æ®ï¼ˆæ–‡æ¡£åˆ—è¡¨ï¼‰
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.documents, f)
        print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
        stats_path = Config.VECTORS_DIR / "stats.json"
        stats = {
            "total_documents": len(self.documents),
            "vector_dimension": self.dimension,
            "index_type": "IndexFlatL2",
            "sources": list(set(doc.source_url for doc in self.documents)),
            "sections": list(set(doc.section_title for doc in self.documents if doc.section_title))
        }

        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")

    def load_index(
        self,
        index_path: Optional[Path] = None,
        metadata_path: Optional[Path] = None
    ):
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•

        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        # é»˜è®¤è·¯å¾„
        if index_path is None:
            index_path = Config.VECTORS_DIR / "index.faiss"
        if metadata_path is None:
            metadata_path = Config.VECTORS_DIR / "metadata.pkl"

        print(f"\nğŸ“‚ åŠ è½½ç´¢å¼•")
        print("-" * 70)

        # åŠ è½½FAISSç´¢å¼•
        if not index_path.exists():
            raise FileNotFoundError(f"Index file not found: {index_path}")

        self.index = faiss.read_index(str(index_path))
        print(f"âœ… FAISSç´¢å¼•å·²åŠ è½½: {index_path}")

        # åŠ è½½å…ƒæ•°æ®
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

        with open(metadata_path, 'rb') as f:
            self.documents = pickle.load(f)
        print(f"âœ… å…ƒæ•°æ®å·²åŠ è½½: {metadata_path}")

        print(f"   æ–‡æ¡£æ•°: {len(self.documents)}")
        print(f"   å‘é‡ç»´åº¦: {self.index.d}")

    def search(
        self,
        query: str,
        top_k: int = None
    ) -> List[tuple[Document, float]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ

        Returns:
            (æ–‡æ¡£, è·ç¦») åˆ—è¡¨
        """
        if self.index is None:
            raise ValueError("Index not loaded")

        top_k = top_k or Config.TOP_K_RESULTS

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_client.embed(query)
        query_vector = np.array([query_embedding], dtype=np.float32)

        # æœç´¢
        distances, indices = self.index.search(query_vector, top_k)

        # è¿”å›ç»“æœ
        results = []
        for distance, idx in zip(distances[0], indices[0]):
            if idx < len(self.documents):
                doc = self.documents[idx]
                # å°†L2è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                # ä½¿ç”¨ 1 / (1 + distance) å°†è·ç¦»æ˜ å°„åˆ° (0, 1]
                similarity = 1.0 / (1.0 + float(distance))
                results.append((doc, similarity))

        return results

    def get_stats(self) -> dict:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if self.index is None or not self.documents:
            return {
                "indexed": False,
                "total_documents": 0
            }

        return {
            "indexed": True,
            "total_documents": len(self.documents),
            "vector_dimension": self.dimension,
            "sources": list(set(doc.source_url for doc in self.documents)),
            "sections": list(set(doc.section_title for doc in self.documents if doc.section_title))
        }


def main():
    """æµ‹è¯•å‡½æ•°"""
    from ..scraper import HTMLCleaner, ContentParser

    print("ğŸ§ª æµ‹è¯•å‘é‡ç´¢å¼•æ„å»ºå™¨")
    print("=" * 70)

    # æ£€æŸ¥APIå¯†é’¥
    if not Config.OPENAI_API_KEY:
        print("âŒ è¯·å…ˆé…ç½® OPENAI_API_KEY")
        return

    # è¯»å–ç¤ºä¾‹HTML
    sample_html_path = Config.PROJECT_ROOT / "data" / "cache" / "sample_legal_content.html"
    if not sample_html_path.exists():
        print(f"âŒ ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {sample_html_path}")
        return

    html = sample_html_path.read_text(encoding='utf-8')

    # è§£æå†…å®¹
    cleaner = HTMLCleaner()
    parser = ContentParser()

    cleaned_html, _ = cleaner.clean_and_extract(html)
    structured = parser.parse(
        html=cleaned_html,
        url="https://example.com/sample",
        title="ç¦»èŒç»æµè¡¥å¿æŒ‡å—"
    )

    # æ„å»ºç´¢å¼•
    indexer = VectorIndexer()
    indexer.build_index([structured], show_progress=True)

    # ä¿å­˜ç´¢å¼•
    indexer.save_index()

    # æµ‹è¯•æœç´¢
    print("\nğŸ” æµ‹è¯•æœç´¢")
    print("=" * 70)

    queries = [
        "å¦‚ä½•è®¡ç®—ç»æµè¡¥å¿é‡‘ï¼Ÿ",
        "N+1è¡¥å¿æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
        "åŠ³åŠ¨ä»²è£éœ€è¦ä»€ä¹ˆææ–™ï¼Ÿ"
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 70)

        results = indexer.search(query, top_k=3)

        for i, (doc, score) in enumerate(results, 1):
            print(f"\nç»“æœ {i} (ç›¸ä¼¼åº¦: {score:.4f})")
            print(f"ç« èŠ‚: {doc.section_title}")
            print(f"å†…å®¹: {doc.content[:100]}...")

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()
