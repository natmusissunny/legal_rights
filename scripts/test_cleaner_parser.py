"""
æµ‹è¯•HTMLæ¸…æ´—å’Œå†…å®¹è§£æ
ä½¿ç”¨ç¤ºä¾‹HTMLæ–‡ä»¶
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root.parent))

from legal_rights.scraper import HTMLCleaner, ContentParser


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯• HTML æ¸…æ´—å’Œå†…å®¹è§£æ")
    print("=" * 80)

    # è¯»å–ç¤ºä¾‹HTML
    sample_html_path = project_root / "data" / "cache" / "sample_legal_content.html"

    if not sample_html_path.exists():
        print(f"âŒ ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {sample_html_path}")
        return

    print(f"\nğŸ“„ è¯»å–ç¤ºä¾‹æ–‡ä»¶: {sample_html_path.name}")
    html = sample_html_path.read_text(encoding='utf-8')
    print(f"   åŸå§‹é•¿åº¦: {len(html):,} å­—ç¬¦")

    # æµ‹è¯• HTML æ¸…æ´—
    print("\n" + "-" * 80)
    print("[æ­¥éª¤1] HTML æ¸…æ´—")
    print("-" * 80)

    cleaner = HTMLCleaner()
    cleaned_html, text = cleaner.clean_and_extract(html)

    print(f"âœ… æ¸…æ´—å®Œæˆ")
    print(f"   æ¸…æ´—å HTML é•¿åº¦: {len(cleaned_html):,} å­—ç¬¦")
    print(f"   æå–æ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
    print(f"   å‹ç¼©ç‡: {(1 - len(cleaned_html)/len(html)) * 100:.1f}%")

    # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
    print(f"\nğŸ“ æå–çš„æ–‡æœ¬é¢„è§ˆ (å‰300å­—):")
    print("-" * 60)
    print(text[:300])
    if len(text) > 300:
        print("...")

    # æµ‹è¯•å†…å®¹è§£æ
    print("\n" + "-" * 80)
    print("[æ­¥éª¤2] å†…å®¹è§£æ")
    print("-" * 80)

    parser = ContentParser()
    structured = parser.parse(
        html=cleaned_html,
        url="https://example.com/sample",
        title=None
    )

    print(f"âœ… è§£æå®Œæˆ")
    print(f"   æ–‡æ¡£æ ‡é¢˜: {structured.title}")
    print(f"   æ¥æºURL: {structured.url}")
    print(f"   æŠ“å–æ—¶é—´: {structured.scraped_at.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   é¡¶å±‚ç« èŠ‚æ•°: {len(structured.sections)}")

    # ç»Ÿè®¡æ€»ç« èŠ‚æ•°
    def count_sections(sections):
        count = len(sections)
        for section in sections:
            count += count_sections(section.subsections)
        return count

    total_sections = count_sections(structured.sections)
    print(f"   æ€»ç« èŠ‚æ•°: {total_sections}")

    # æ˜¾ç¤ºç« èŠ‚ç»“æ„
    print(f"\nğŸ“š ç« èŠ‚ç»“æ„:")
    print("-" * 60)
    print_section_tree(structured.sections, indent=0)

    # æ˜¾ç¤ºè¯¦ç»†å†…å®¹
    print("\n" + "-" * 80)
    print("[æ­¥éª¤3] è¯¦ç»†å†…å®¹å±•ç¤º")
    print("-" * 80)

    if structured.sections:
        first_section = structured.sections[0]
        print(f"\nç¤ºä¾‹ç« èŠ‚: {first_section.title}")
        print(f"å±‚çº§: {first_section.level}")
        print(f"å†…å®¹é•¿åº¦: {len(first_section.content)} å­—ç¬¦")
        print(f"å­ç« èŠ‚æ•°: {len(first_section.subsections)}")
        print(f"\nå†…å®¹é¢„è§ˆ:")
        print("-" * 60)
        content_preview = first_section.content[:400]
        print(content_preview)
        if len(first_section.content) > 400:
            print("...")

    # æµ‹è¯•å…³é”®è¯æå–
    print("\n" + "-" * 80)
    print("[æ­¥éª¤4] å…³é”®è¯æå–")
    print("-" * 80)

    keywords = parser.extract_keywords(text)
    print(f"âœ… æå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯:")
    print(", ".join(keywords[:15]))  # æ˜¾ç¤ºå‰15ä¸ª

    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("=" * 80)


def print_section_tree(sections, indent=0):
    """æ‰“å°ç« èŠ‚æ ‘å½¢ç»“æ„"""
    for i, section in enumerate(sections):
        prefix = "â”‚   " * indent + "â”œâ”€â”€ "
        if i == len(sections) - 1:
            prefix = "â”‚   " * indent + "â””â”€â”€ "

        # å†…å®¹é¢„è§ˆ
        content_preview = ""
        if section.content:
            preview_text = section.content[:50].replace('\n', ' ')
            content_preview = f" [{preview_text}...]"

        print(f"{prefix}[Lv{section.level}] {section.title}{content_preview}")

        if section.subsections:
            print_section_tree(section.subsections, indent + 1)


if __name__ == "__main__":
    main()
